{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Approaches to Recommendation Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recommender Systems are one of the most popular applications of Machine Learning systems. Due to their widespread success, they are quickly becoming ubiquitous to a lot of businesses. Traditionally, collaborative filtering and matrix factorization techniques were used to solve these problems.\n",
    "\n",
    "In the last couple of years, this trend has been changing. Due to the massive success of effectively training deep neural nets, new approaches have been developed by leveraging the tools and modeling flexibility from the Deep Learning ecosystem.\n",
    "\n",
    "This hack session gives a primer into these concepts using neural network architectures.\n",
    "\n",
    "For those who are interested in an intuitive explanation to collaborative filtering and embeddings, please refer to this brilliant **fast.ai lesson by Jeremy and Rachel - http://course.fast.ai/lessons/lesson4.html**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recommendation Engines\n",
    "============\n",
    "\n",
    "## Common Applications\n",
    "    - Feed (News Feed on Facebook, Feed on Twitter, Explore on Instagram, Home Page of Amazon etc.)\n",
    "    - Rule of thumb: \n",
    "        - Large item inventory\n",
    "        - Thus discoverability is a problem.\n",
    "        - Recsys will make their mark.\n",
    "    - Traditional methods in recsys\n",
    "        - User - User similarity\n",
    "        - Item - Item similarity\n",
    "        - Hybrid models - Collaborative Filtering\n",
    "        - Matrix factorization\n",
    "    - Entry of Neural Approaches\n",
    "        - Latent factors in earlier approaches analogous to Embeddings in deep learning ecosystem\n",
    "        - GPU training, superior optimization techniques (Adam etc.) enabled neural approaches to be preferred.\n",
    "        - Flexibility of adding layers, ease of adding additional metadata and joint-training is a plus in this approach.\n",
    "\n",
    "\n",
    "### Let's dive!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table of Contents:\n",
    "    0. Installations\n",
    "    1. Import the necessary libraries (print versions of the libraries)\n",
    "    2. Read the necessary datasets\n",
    "    3. Create the interactions frame\n",
    "    4. Split the frame into train and validation sets\n",
    "    5. Create the keras network (after creating necessary embeddings)\n",
    "    6. Train the network and monitor accuracy on validation\n",
    "    7. Make the network deeper by adding dense layers and re-train the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installations\n",
    "    - wget https://repo.continuum.io/archive/Anaconda3-5.0.0-MacOSX-x86_64.sh # Anaconda Python 3.6 installer\n",
    "    - conda install -c conda-forge keras # Install Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Pandas version: ', u'0.18.1')\n",
      "('Numpy version: ', '1.11.1')\n",
      "('Keras version: ', '1.1.0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: Tesla K80 (CNMeM is disabled, cuDNN 5103)\n",
      "/home/ubuntu/anaconda2/lib/python2.7/site-packages/theano/sandbox/cuda/__init__.py:600: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.\n",
      "  warnings.warn(warn)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Setting seed before importing keras to ensure reproducibility\n",
    "np.random.seed(2017)\n",
    "import keras as K\n",
    "\n",
    "print(\"Pandas version: \", pd.__version__)\n",
    "print(\"Numpy version: \", np.__version__)\n",
    "print(\"Keras version: \", K.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Read the necessary datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(679051, 4)\n",
      "(291023, 3)\n",
      "# Users: 73489 | # Articles: 214027\n"
     ]
    }
   ],
   "source": [
    "# Reading in the datasets\n",
    "train = pd.read_csv(\"../input/train.csv\"); print(train.shape)\n",
    "test = pd.read_csv(\"../input/test.csv\"); print(test.shape)\n",
    "\n",
    "print(\"# Users: {} | # Articles: {}\".format(train.User_ID.nunique(), train.Article_ID.nunique()))\n",
    "\n",
    "diff = np.setdiff1d(train.User_ID.unique(), test.User_ID.unique())\n",
    "train = train[~train.User_ID.isin(diff)].reset_index(drop=True) # Drop train-only users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Create the interactions frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creating one dataframe of the interactions\n",
    "ratings = pd.concat([train, test])\n",
    "\n",
    "users = ratings.User_ID.unique()\n",
    "articles = ratings.Article_ID.unique()\n",
    "\n",
    "# Create userid & itemid to index mappings\n",
    "userid2idx = {o:i for i,o in enumerate(users)}\n",
    "articlesid2idx = {o:i for i,o in enumerate(articles)}\n",
    "\n",
    "ratings.Article_ID = ratings.Article_ID.apply(lambda x: articlesid2idx[x])\n",
    "ratings.User_ID = ratings.User_ID.apply(lambda x: userid2idx[x])\n",
    "\n",
    "n_users = ratings.User_ID.nunique()\n",
    "n_articles = ratings.Article_ID.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Split the frame into train and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(618437, 4)\n",
      "(291023, 4)\n"
     ]
    }
   ],
   "source": [
    "X_train = ratings[0:len(train)]; print(X_train.shape)\n",
    "X_test = ratings[len(train):len(ratings)]; print(X_test.shape)\n",
    "\n",
    "# Split the data into train and validation sets\n",
    "np.random.seed(2017)\n",
    "msk = np.random.rand(len(X_train)) < 0.8\n",
    "trn = X_train[msk]\n",
    "val = X_train[~msk]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Create the keras network \n",
    "    - After creating necessary embeddings for each User_ID and Article_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_factors = 50\n",
    "import keras.backend as K\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    score = K.sqrt(K.mean(K.pow(y_true - y_pred, 2)))\n",
    "    return score\n",
    "\n",
    "from keras.layers import Input, Embedding, Dense, Dropout, merge, Flatten\n",
    "from keras.regularizers import l2\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import Callback, TensorBoard\n",
    "\n",
    "def embedding_input(name, n_in, n_out, reg):\n",
    "    inp = Input(shape=(1,), dtype='int64', name=name)\n",
    "    return inp, Embedding(n_in, n_out, input_length=1, W_regularizer=l2(reg))(inp)\n",
    "\n",
    "def create_bias(inp, n_in):\n",
    "    x = Embedding(n_in, 1, input_length=1)(inp)\n",
    "    return Flatten()(x)\n",
    "\n",
    "user_in, u = embedding_input('user_in', n_users, n_factors, 1e-5)\n",
    "article_in, a = embedding_input('article_in', n_articles, n_factors, 1e-5)\n",
    "\n",
    "ub = create_bias(user_in, n_users)\n",
    "ab = create_bias(article_in, n_articles)\n",
    "\n",
    "x = merge([u, a], mode='dot')\n",
    "x = Flatten()(x)\n",
    "x = merge([x, ub], mode='sum')\n",
    "x = merge([x, ab], mode='sum')\n",
    "\n",
    "model = Model([user_in, article_in], x)\n",
    "model.compile(Adam(5e-3), loss='mse', metrics=[rmse])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 6. Train the network and monitor accuracy on validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 494544 samples, validate on 123893 samples\n",
      "Epoch 1/10\n",
      "494544/494544 [==============================] - 25s - loss: 5.1717 - rmse: 2.2598 - val_loss: 4.6196 - val_rmse: 2.1485\n",
      "Epoch 2/10\n",
      "494544/494544 [==============================] - 25s - loss: 4.1626 - rmse: 1.9570 - val_loss: 4.0984 - val_rmse: 2.0236\n",
      "Epoch 3/10\n",
      "494544/494544 [==============================] - 25s - loss: 3.0437 - rmse: 1.5325 - val_loss: 3.9732 - val_rmse: 1.9925\n",
      "Epoch 4/10\n",
      "494544/494544 [==============================] - 25s - loss: 2.4167 - rmse: 1.2179 - val_loss: 3.9204 - val_rmse: 1.9792\n",
      "Epoch 5/10\n",
      "494544/494544 [==============================] - 25s - loss: 2.1524 - rmse: 1.0553 - val_loss: 3.8827 - val_rmse: 1.9696\n",
      "Epoch 6/10\n",
      "494544/494544 [==============================] - 25s - loss: 2.0053 - rmse: 0.9676 - val_loss: 3.8420 - val_rmse: 1.9593\n",
      "Epoch 7/10\n",
      "494544/494544 [==============================] - 25s - loss: 1.9005 - rmse: 0.9141 - val_loss: 3.8089 - val_rmse: 1.9508\n",
      "Epoch 8/10\n",
      "494544/494544 [==============================] - 25s - loss: 1.8143 - rmse: 0.8768 - val_loss: 3.7790 - val_rmse: 1.9432\n",
      "Epoch 9/10\n",
      "494544/494544 [==============================] - 25s - loss: 1.7417 - rmse: 0.8496 - val_loss: 3.7589 - val_rmse: 1.9380\n",
      "Epoch 10/10\n",
      "494544/494544 [==============================] - 25s - loss: 1.6766 - rmse: 0.8276 - val_loss: 3.7456 - val_rmse: 1.9346\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f72322fa650>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([trn.User_ID, trn.Article_ID], trn.Rating,\n",
    "          nb_epoch=10, batch_size=1024,\n",
    "          validation_data=([val.User_ID, val.Article_ID], val.Rating))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Make the network deeper by adding dense layers and re-train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 494544 samples, validate on 123893 samples\n",
      "Epoch 1/10\n",
      "494544/494544 [==============================] - 12s - loss: 3.8499 - rmse: 1.9420 - val_loss: 3.3099 - val_rmse: 1.8189\n",
      "Epoch 2/10\n",
      "494544/494544 [==============================] - 12s - loss: 3.4671 - rmse: 1.8049 - val_loss: 3.3192 - val_rmse: 1.8215\n",
      "Epoch 3/10\n",
      "494544/494544 [==============================] - 12s - loss: 3.2863 - rmse: 1.7276 - val_loss: 3.3685 - val_rmse: 1.8350\n",
      "Epoch 4/10\n",
      "494544/494544 [==============================] - 12s - loss: 3.2012 - rmse: 1.6841 - val_loss: 3.4301 - val_rmse: 1.8516\n",
      "Epoch 5/10\n",
      "494544/494544 [==============================] - 12s - loss: 3.1671 - rmse: 1.6602 - val_loss: 3.4676 - val_rmse: 1.8617\n",
      "Epoch 6/10\n",
      "494544/494544 [==============================] - 12s - loss: 3.1606 - rmse: 1.6490 - val_loss: 3.4886 - val_rmse: 1.8674\n",
      "Epoch 7/10\n",
      "494544/494544 [==============================] - 12s - loss: 3.1543 - rmse: 1.6411 - val_loss: 3.4903 - val_rmse: 1.8678\n",
      "Epoch 8/10\n",
      "494544/494544 [==============================] - 12s - loss: 3.1509 - rmse: 1.6357 - val_loss: 3.4897 - val_rmse: 1.8676\n",
      "Epoch 9/10\n",
      "494544/494544 [==============================] - 12s - loss: 3.1525 - rmse: 1.6333 - val_loss: 3.5102 - val_rmse: 1.8731\n",
      "Epoch 10/10\n",
      "494544/494544 [==============================] - 12s - loss: 3.1521 - rmse: 1.6311 - val_loss: 3.5123 - val_rmse: 1.8737\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7211ac3d50>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_in, u = embedding_input('user_in', n_users, n_factors, 1e-5)\n",
    "article_in, a = embedding_input('article_in', n_articles, n_factors, 1e-5)\n",
    "\n",
    "ub = create_bias(user_in, n_users)\n",
    "ab = create_bias(article_in, n_articles)\n",
    "\n",
    "x = merge([u, a], mode='concat')\n",
    "x = Flatten()(x)\n",
    "\n",
    "# Dense connections\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(20, activation='relu')(x)\n",
    "x = Dropout(0.75)(x)\n",
    "x = Dense(1)(x)\n",
    "\n",
    "model = Model([user_in, article_in], x)\n",
    "model.compile(Adam(5e-3), loss='mse', metrics=[rmse])\n",
    "\n",
    "model.fit([trn.User_ID, trn.Article_ID], trn.Rating,\n",
    "          nb_epoch=10, batch_size=2048,\n",
    "          validation_data=([val.User_ID, val.Article_ID], val.Rating))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 2048 - 13s - loss: 2.9813 - rmse: 1.5600 - val_loss: 3.5050 - val_rmse: 1.8718\n",
    "# 1024 - 25s - loss: 3.2050 - rmse: 1.6015 - val_loss: 3.4731 - val_rmse: 1.8629"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using side information\n",
    "\n",
    "Often, along with the user-interaction data, other information such as user metadata and item metadata is also given. With the above networks, it's trivial to add this metadata to our model. Let's see how."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(679051, 4)\n",
      "(291023, 3)\n",
      "(618437, 6)\n",
      "(618437, 9)\n",
      "(291023, 5)\n",
      "(291023, 8)\n",
      "(618437, 9)\n",
      "(291023, 9)\n"
     ]
    }
   ],
   "source": [
    "# 2. Read the necessary datasets\n",
    "user = pd.read_csv(\"../input/user.csv\")\n",
    "article = pd.read_csv(\"../input/article.csv\")\n",
    "train = pd.read_csv(\"../input/train.csv\"); print(train.shape)\n",
    "test = pd.read_csv(\"../input/test.csv\"); print(test.shape)\n",
    "\n",
    "diff = np.setdiff1d(train.User_ID.unique(), test.User_ID.unique())\n",
    "train = train[~train.User_ID.isin(diff)].reset_index(drop=True) # Drop train-only users.\n",
    "\n",
    "train = train.merge(user, how='left'); print(train.shape)\n",
    "train = train.merge(article, how='left'); print(train.shape)\n",
    "test = test.merge(user, how='left'); print(test.shape)\n",
    "test = test.merge(article, how='left'); print(test.shape)\n",
    "\n",
    "\n",
    "\n",
    "# For simplicity, impute with 0.\n",
    "# Ideally, you should either do mean / median imputation for numeric vars & mode imputation for cat vars.\n",
    "train = train.fillna(0)\n",
    "test = test.fillna(0)\n",
    "\n",
    "\n",
    "# 3. Create the interactions frame\n",
    "ratings = pd.concat([train, test])\n",
    "\n",
    "# Scaling numeric columns\n",
    "from sklearn.preprocessing import scale\n",
    "ratings.VintageMonths = scale(ratings.VintageMonths)\n",
    "\n",
    "users = ratings.User_ID.unique()\n",
    "articles = ratings.Article_ID.unique()\n",
    "age = ratings.Age.unique()\n",
    "var1 = ratings.Var1.unique()\n",
    "\n",
    "# Create userid & itemid to index mappings\n",
    "userid2idx = {o:i for i,o in enumerate(users)}\n",
    "articlesid2idx = {o:i for i,o in enumerate(articles)}\n",
    "age2idx = {o:i for i,o in enumerate(age)}\n",
    "var12idx = {o:i for i,o in enumerate(var1)}\n",
    "\n",
    "ratings.Article_ID = ratings.Article_ID.apply(lambda x: articlesid2idx[x])\n",
    "ratings.User_ID = ratings.User_ID.apply(lambda x: userid2idx[x])\n",
    "ratings.Age = ratings.Age.apply(lambda x: age2idx[x])\n",
    "ratings.Var1 = ratings.Var1.apply(lambda x: var12idx[x])\n",
    "\n",
    "n_users = ratings.User_ID.nunique()\n",
    "n_articles = ratings.Article_ID.nunique()\n",
    "n_age = ratings.Age.nunique()\n",
    "n_var1 = ratings.Var1.nunique()\n",
    "\n",
    "\n",
    "# 4. Split the frame into train and validation sets\n",
    "X_train = ratings[0:len(train)]; print(X_train.shape)\n",
    "X_test = ratings[len(train):len(ratings)]; print(X_test.shape)\n",
    "\n",
    "# Split the data into train and validation sets\n",
    "np.random.seed(2017)\n",
    "msk = np.random.rand(len(X_train)) < 0.8\n",
    "trn = X_train[msk]\n",
    "val = X_train[~msk]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add age and other numeric variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 494544 samples, validate on 123893 samples\n",
      "Epoch 1/10\n",
      "494544/494544 [==============================] - 13s - loss: 3.6028 - rmse: 1.8727 - val_loss: 3.2393 - val_rmse: 1.7994\n",
      "Epoch 2/10\n",
      "494544/494544 [==============================] - 13s - loss: 3.1278 - rmse: 1.6928 - val_loss: 3.3074 - val_rmse: 1.8182\n",
      "Epoch 3/10\n",
      "494544/494544 [==============================] - 13s - loss: 2.7962 - rmse: 1.5533 - val_loss: 3.4844 - val_rmse: 1.8662\n",
      "Epoch 4/10\n",
      "494544/494544 [==============================] - 13s - loss: 2.5877 - rmse: 1.4673 - val_loss: 3.5905 - val_rmse: 1.8945\n",
      "Epoch 5/10\n",
      "494544/494544 [==============================] - 13s - loss: 2.4664 - rmse: 1.4096 - val_loss: 3.7130 - val_rmse: 1.9265\n",
      "Epoch 6/10\n",
      "494544/494544 [==============================] - 13s - loss: 2.3786 - rmse: 1.3625 - val_loss: 3.7056 - val_rmse: 1.9246\n",
      "Epoch 7/10\n",
      "494544/494544 [==============================] - 13s - loss: 2.3156 - rmse: 1.3253 - val_loss: 3.7644 - val_rmse: 1.9398\n",
      "Epoch 8/10\n",
      "494544/494544 [==============================] - 13s - loss: 2.2632 - rmse: 1.2943 - val_loss: 3.8164 - val_rmse: 1.9532\n",
      "Epoch 9/10\n",
      "494544/494544 [==============================] - 13s - loss: 2.2146 - rmse: 1.2670 - val_loss: 3.7941 - val_rmse: 1.9474\n",
      "Epoch 10/10\n",
      "494544/494544 [==============================] - 13s - loss: 2.1804 - rmse: 1.2474 - val_loss: 3.8137 - val_rmse: 1.9524\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3d2b30c710>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO\n",
    "user_in, u = embedding_input('user_in', n_users, n_factors, 1e-5)\n",
    "article_in, a = embedding_input('article_in', n_articles, n_factors, 1e-5)\n",
    "\n",
    "meta_input_f0 = Input(shape=[1], name='meta_input_f0') # Age\n",
    "meta_input_f1 = Input(shape=[1], name='meta_input_f1') # NumberOfArticlesBySameAuthor\n",
    "meta_input_f2 = Input(shape=[1], name='meta_input_f2') # NumberOfArticlesinSameCategory\n",
    "meta_input_f3 = Input(shape=[1], name='meta_input_f3') # VintageMonths\n",
    "meta_input_f4 = Input(shape=[1], name='meta_input_f4') # Var1\n",
    "\n",
    "ub = create_bias(user_in, n_users)\n",
    "ab = create_bias(article_in, n_articles)\n",
    "\n",
    "x = merge([u, a], mode='concat')\n",
    "x = Flatten()(x)\n",
    "\n",
    "# Dense connections\n",
    "x = Dropout(0.2)(x)\n",
    "x = Dense(50, activation='relu')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Dense(1)(x)\n",
    "\n",
    "model = Model([user_in, article_in, meta_input_f0, meta_input_f1, meta_input_f2, meta_input_f3, meta_input_f4], x)\n",
    "model.compile(Adam(5e-3), loss='mse', metrics=[rmse])\n",
    "\n",
    "model.fit([trn.User_ID, trn.Article_ID, trn.Age, trn['NumberOfArticlesBySameAuthor\\r'], trn['NumberOfArticlesinSameCategory\\r'], trn['VintageMonths'], trn['Var1']], trn.Rating,\n",
    "          nb_epoch=10, batch_size=2048,\n",
    "          validation_data=([val.User_ID, val.Article_ID, val.Age, val['NumberOfArticlesBySameAuthor\\r'], val['NumberOfArticlesinSameCategory\\r'], val['VintageMonths'], val['Var1']], val.Rating))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 13s - loss: 3.7780 - rmse: 1.9228 - val_loss: 3.2853 - val_rmse: 1.8121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 494544 samples, validate on 123893 samples\n",
      "Epoch 1/10\n",
      "494544/494544 [==============================] - 13s - loss: 3.7576 - rmse: 1.9179 - val_loss: 3.2908 - val_rmse: 1.8136\n",
      "Epoch 2/10\n",
      "494544/494544 [==============================] - 12s - loss: 3.3547 - rmse: 1.7715 - val_loss: 3.2690 - val_rmse: 1.8076\n",
      "Epoch 3/10\n",
      "494544/494544 [==============================] - 13s - loss: 3.1504 - rmse: 1.6815 - val_loss: 3.3650 - val_rmse: 1.8340\n",
      "Epoch 4/10\n",
      "494544/494544 [==============================] - 12s - loss: 3.0426 - rmse: 1.6260 - val_loss: 3.4146 - val_rmse: 1.8475\n",
      "Epoch 5/10\n",
      "494544/494544 [==============================] - 12s - loss: 2.9964 - rmse: 1.5957 - val_loss: 3.5024 - val_rmse: 1.8710\n",
      "Epoch 6/10\n",
      "494544/494544 [==============================] - 13s - loss: 2.9748 - rmse: 1.5783 - val_loss: 3.4866 - val_rmse: 1.8668\n",
      "Epoch 7/10\n",
      "494544/494544 [==============================] - 13s - loss: 2.9676 - rmse: 1.5691 - val_loss: 3.4986 - val_rmse: 1.8700\n",
      "Epoch 8/10\n",
      "494544/494544 [==============================] - 13s - loss: 2.9575 - rmse: 1.5611 - val_loss: 3.5152 - val_rmse: 1.8744\n",
      "Epoch 9/10\n",
      "494544/494544 [==============================] - 13s - loss: 2.9589 - rmse: 1.5585 - val_loss: 3.5039 - val_rmse: 1.8714\n",
      "Epoch 10/10\n",
      "494544/494544 [==============================] - 12s - loss: 2.9563 - rmse: 1.5549 - val_loss: 3.5492 - val_rmse: 1.8835\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3d2b04da90>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO\n",
    "user_in, u = embedding_input('user_in', n_users, n_factors, 1e-5)\n",
    "article_in, a = embedding_input('article_in', n_articles, n_factors, 1e-5)\n",
    "\n",
    "meta_input_f0 = Input(shape=[1], name='meta_input_f0') # Age\n",
    "meta_input_f1 = Input(shape=[1], name='meta_input_f1') # NumberOfArticlesBySameAuthor\n",
    "meta_input_f2 = Input(shape=[1], name='meta_input_f2') # NumberOfArticlesinSameCategory\n",
    "meta_input_f3 = Input(shape=[1], name='meta_input_f3') # VintageMonths\n",
    "meta_input_f4 = Input(shape=[1], name='meta_input_f4') # Var1\n",
    "\n",
    "ub = create_bias(user_in, n_users)\n",
    "ab = create_bias(article_in, n_articles)\n",
    "\n",
    "x = merge([Flatten()(u), Flatten()(a), meta_input_f0], mode='concat')\n",
    "# x = Flatten()(x)\n",
    "\n",
    "# Dense connections\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(50, activation='relu')(x)\n",
    "x = Dropout(0.75)(x)\n",
    "x = Dense(1)(x)\n",
    "\n",
    "model = Model([user_in, article_in, meta_input_f0], x)\n",
    "model.compile(Adam(5e-3), loss='mse', metrics=[rmse])\n",
    "\n",
    "model.fit([trn.User_ID, trn.Article_ID, trn.Age], trn.Rating,\n",
    "          nb_epoch=10, batch_size=2048,\n",
    "          validation_data=([val.User_ID, val.Article_ID, val.Age], val.Rating))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credit where it's due\n",
    "    - A brilliant fast.ai course by Jeremy and Rachel. Refer to Lesson 4 for Collaborative Filtering lecture.\n",
    "    - https://m2dsupsdlclass.github.io/lectures-labs/\n",
    "    - Deep Recommender models using PyTorch - https://github.com/maciejkula/spotlight\n",
    "    - YouTube Recommendation Engine - http://static.googleusercontent.com/media/research.google.com/en//pubs/archive/45530.pdf\n",
    "    - Keras Merge Layer - https://faroit.github.io/keras-docs/1.0.4/getting-started/sequential-model-guide/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
