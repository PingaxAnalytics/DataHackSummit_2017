{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Approaches to Recommendation Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recommender Systems are one of the most popular applications of Machine Learning systems. Due to their widespread success, they are quickly becoming ubiquitous to a lot of businesses. Traditionally, collaborative filtering and matrix factorization techniques were used to solve these problems.\n",
    "\n",
    "In the last couple of years, this trend has been changing. Due to the massive success of effectively training deep neural nets, new approaches have been developed by leveraging the tools and modeling flexibility from the Deep Learning ecosystem.\n",
    "\n",
    "This hack session gives a primer into these concepts using neural network architectures.\n",
    "\n",
    "For those who are interested in an intuitive explanation to collaborative filtering and embeddings, please refer to this brilliant **fast.ai lesson by Jeremy and Rachel - http://course.fast.ai/lessons/lesson4.html**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendation Engines\n",
    "\n",
    "\n",
    "## Common Applications\n",
    "    - Feed (News Feed on Facebook, Feed on Twitter, Explore on Instagram, Home Page of Amazon etc.)\n",
    "    - Rule of thumb: \n",
    "        - Large item inventory\n",
    "        - Thus discoverability is a problem.\n",
    "        - Recsys will make their mark.\n",
    "    - Traditional methods in recsys\n",
    "        - User - User similarity\n",
    "        - Item - Item similarity\n",
    "        - Hybrid models - Collaborative Filtering\n",
    "        - Matrix factorization\n",
    "    - Entry of Neural Approaches\n",
    "        - Latent factors in earlier approaches analogous to Embeddings in deep learning ecosystem\n",
    "        - GPU training, superior optimization techniques (Adam etc).\n",
    "        - Flexibility of adding layers, ease of adding additional metadata and joint-training is a plus in this approach.\n",
    "\n",
    "\n",
    "### Let's dive!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table of Contents:\n",
    "    0. Installations\n",
    "    1. Import the necessary libraries (print versions of the libraries)\n",
    "    2. Read the necessary datasets\n",
    "    3. Create the interactions frame\n",
    "    4. Split the frame into train and validation sets\n",
    "    5. Create the keras network (after creating necessary embeddings)\n",
    "    6. Train the network and monitor accuracy on validation\n",
    "    7. Make the network deeper by adding dense layers and re-train the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Installations\n",
    "    - wget https://repo.continuum.io/archive/Anaconda3-5.0.1-Linux-x86_64.sh # Anaconda Python 3.6 installer\n",
    "    - conda install -c conda-forge keras # Install Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using gpu device 0: Tesla K80 (CNMeM is disabled, cuDNN 5103)\n",
      "/home/ubuntu/anaconda2/lib/python2.7/site-packages/theano/sandbox/cuda/__init__.py:600: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.\n",
      "  warnings.warn(warn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Pandas version: ', u'0.18.1')\n",
      "('Numpy version: ', '1.11.1')\n",
      "('Keras version: ', '1.1.0')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Setting seed before importing keras to ensure reproducibility\n",
    "np.random.seed(2017)\n",
    "import keras as K\n",
    "\n",
    "print(\"Pandas version: \", pd.__version__)\n",
    "print(\"Numpy version: \", np.__version__)\n",
    "print(\"Keras version: \", K.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Read the necessary datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(679051, 4)\n",
      "(291023, 3)\n",
      "# Users: 73489 | # Articles: 214027\n"
     ]
    }
   ],
   "source": [
    "# Reading in the datasets\n",
    "train = pd.read_csv(\"../input/train.csv\"); print(train.shape)\n",
    "test = pd.read_csv(\"../input/test.csv\"); print(test.shape)\n",
    "\n",
    "print(\"# Users: {} | # Articles: {}\".format(train.User_ID.nunique(), train.Article_ID.nunique()))\n",
    "\n",
    "diff = np.setdiff1d(train.User_ID.unique(), test.User_ID.unique())\n",
    "train = train[~train.User_ID.isin(diff)].reset_index(drop=True) # Drop train-only users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User_ID</th>\n",
       "      <th>Article_ID</th>\n",
       "      <th>Rating</th>\n",
       "      <th>ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20080828074</td>\n",
       "      <td>1219102233</td>\n",
       "      <td>0</td>\n",
       "      <td>20080828074_1219102233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20080820760</td>\n",
       "      <td>1219151095</td>\n",
       "      <td>0</td>\n",
       "      <td>20080820760_1219151095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20080824760</td>\n",
       "      <td>1219295837</td>\n",
       "      <td>5</td>\n",
       "      <td>20080824760_1219295837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20080820470</td>\n",
       "      <td>1219098705</td>\n",
       "      <td>0</td>\n",
       "      <td>20080820470_1219098705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20080821438</td>\n",
       "      <td>1219144384</td>\n",
       "      <td>0</td>\n",
       "      <td>20080821438_1219144384</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       User_ID  Article_ID  Rating                      ID\n",
       "0  20080828074  1219102233       0  20080828074_1219102233\n",
       "1  20080820760  1219151095       0  20080820760_1219151095\n",
       "2  20080824760  1219295837       5  20080824760_1219295837\n",
       "3  20080820470  1219098705       0  20080820470_1219098705\n",
       "4  20080821438  1219144384       0  20080821438_1219144384"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Create the interactions frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creating one dataframe of the interactions\n",
    "ratings = pd.concat([train, test])\n",
    "\n",
    "users = ratings.User_ID.unique() # unique users\n",
    "articles = ratings.Article_ID.unique()\n",
    "\n",
    "# Create userid & itemid to index mappings\n",
    "userid2idx = {o:i for i,o in enumerate(users)}\n",
    "articlesid2idx = {o:i for i,o in enumerate(articles)}\n",
    "\n",
    "ratings.Article_ID = ratings.Article_ID.apply(lambda x: articlesid2idx[x])\n",
    "ratings.User_ID = ratings.User_ID.apply(lambda x: userid2idx[x])\n",
    "\n",
    "n_users = ratings.User_ID.nunique()\n",
    "n_articles = ratings.Article_ID.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User_ID</th>\n",
       "      <th>Article_ID</th>\n",
       "      <th>Rating</th>\n",
       "      <th>ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20080828074</td>\n",
       "      <td>1219102233</td>\n",
       "      <td>0</td>\n",
       "      <td>20080828074_1219102233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20080820760</td>\n",
       "      <td>1219151095</td>\n",
       "      <td>0</td>\n",
       "      <td>20080820760_1219151095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20080824760</td>\n",
       "      <td>1219295837</td>\n",
       "      <td>5</td>\n",
       "      <td>20080824760_1219295837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20080820470</td>\n",
       "      <td>1219098705</td>\n",
       "      <td>0</td>\n",
       "      <td>20080820470_1219098705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20080821438</td>\n",
       "      <td>1219144384</td>\n",
       "      <td>0</td>\n",
       "      <td>20080821438_1219144384</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       User_ID  Article_ID  Rating                      ID\n",
       "0  20080828074  1219102233       0  20080828074_1219102233\n",
       "1  20080820760  1219151095       0  20080820760_1219151095\n",
       "2  20080824760  1219295837       5  20080824760_1219295837\n",
       "3  20080820470  1219098705       0  20080820470_1219098705\n",
       "4  20080821438  1219144384       0  20080821438_1219144384"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Split the frame into train and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(618437, 4)\n",
      "(291023, 4)\n"
     ]
    }
   ],
   "source": [
    "X_train = ratings[0:len(train)]; print(X_train.shape)\n",
    "X_test = ratings[len(train):len(ratings)]; print(X_test.shape)\n",
    "\n",
    "# Split the data into train and validation sets\n",
    "np.random.seed(2017)\n",
    "msk = np.random.rand(len(X_train)) < 0.8 # 20 %\n",
    "trn = X_train[msk]\n",
    "val = X_train[~msk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "253933"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings.Article_ID.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Create the keras network \n",
    "    - After creating necessary embeddings for each User_ID and Article_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_factors = 50\n",
    "import keras.backend as K\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    score = K.sqrt(K.mean(K.pow(y_true - y_pred, 2)))\n",
    "    return score\n",
    "\n",
    "from keras.layers import Input, Embedding, Dense, Dropout, merge, Flatten\n",
    "from keras.regularizers import l2\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import Callback, TensorBoard\n",
    "\n",
    "def embedding_input(name, n_in, n_out, reg):\n",
    "    inp = Input(shape=(1,), dtype='int64', name=name)\n",
    "    return inp, Embedding(n_in, n_out, input_length=1, W_regularizer=l2(reg))(inp)\n",
    "\n",
    "def create_bias(inp, n_in):\n",
    "    x = Embedding(n_in, 1, input_length=1)(inp)\n",
    "    return Flatten()(x)\n",
    "\n",
    "user_in, u = embedding_input('user_in', n_users, n_factors, 1e-3)\n",
    "article_in, a = embedding_input('article_in', n_articles, n_factors, 1e-3)\n",
    "\n",
    "ub = create_bias(user_in, n_users)\n",
    "ab = create_bias(article_in, n_articles)\n",
    "\n",
    "x = merge([u, a], mode='dot')\n",
    "x = Flatten()(x)\n",
    "x = merge([x, ub], mode='sum')\n",
    "x = merge([x, ab], mode='sum')\n",
    "\n",
    "model = Model([user_in, article_in], x)\n",
    "model.compile(Adam(5e-3), loss='mse', metrics=[rmse])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "user_in (InputLayer)             (None, 1)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "article_in (InputLayer)          (None, 1)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)          (None, 1, 50)         2296950     user_in[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)          (None, 1, 50)         12696650    article_in[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "merge_1 (Merge)                  (None, 1, 1)          0           embedding_1[0][0]                \n",
      "                                                                   embedding_2[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)          (None, 1, 1)          45939       user_in[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)              (None, 1)             0           merge_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)              (None, 1)             0           embedding_3[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)          (None, 1, 1)          253933      article_in[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "merge_2 (Merge)                  (None, 1)             0           flatten_3[0][0]                  \n",
      "                                                                   flatten_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)              (None, 1)             0           embedding_4[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "merge_3 (Merge)                  (None, 1)             0           merge_2[0][0]                    \n",
      "                                                                   flatten_2[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 15293472\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 6. Train the network and monitor accuracy on validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 494544 samples, validate on 123893 samples\n",
      "Epoch 1/10\n",
      "494544/494544 [==============================] - 3s - loss: 2.6306 - rmse: 1.6218 - val_loss: 3.5696 - val_rmse: 1.8893\n",
      "Epoch 2/10\n",
      "494544/494544 [==============================] - 3s - loss: 2.6108 - rmse: 1.6157 - val_loss: 3.5659 - val_rmse: 1.8883\n",
      "Epoch 3/10\n",
      "494544/494544 [==============================] - 3s - loss: 2.5913 - rmse: 1.6097 - val_loss: 3.5626 - val_rmse: 1.8874\n",
      "Epoch 4/10\n",
      "494544/494544 [==============================] - 3s - loss: 2.5722 - rmse: 1.6037 - val_loss: 3.5599 - val_rmse: 1.8867\n",
      "Epoch 5/10\n",
      "494544/494544 [==============================] - 3s - loss: 2.5534 - rmse: 1.5978 - val_loss: 3.5572 - val_rmse: 1.8860\n",
      "Epoch 6/10\n",
      "494544/494544 [==============================] - 3s - loss: 2.5349 - rmse: 1.5920 - val_loss: 3.5550 - val_rmse: 1.8854\n",
      "Epoch 7/10\n",
      "494544/494544 [==============================] - 3s - loss: 2.5168 - rmse: 1.5864 - val_loss: 3.5532 - val_rmse: 1.8849\n",
      "Epoch 8/10\n",
      "494544/494544 [==============================] - 3s - loss: 2.4989 - rmse: 1.5807 - val_loss: 3.5516 - val_rmse: 1.8845\n",
      "Epoch 9/10\n",
      "494544/494544 [==============================] - 3s - loss: 2.4813 - rmse: 1.5752 - val_loss: 3.5503 - val_rmse: 1.8842\n",
      "Epoch 10/10\n",
      "494544/494544 [==============================] - 3s - loss: 2.4640 - rmse: 1.5696 - val_loss: 3.5494 - val_rmse: 1.8839\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f81f358ce90>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([trn.User_ID, trn.Article_ID], trn.Rating,\n",
    "          nb_epoch=10, batch_size=8192,\n",
    "          validation_data=([val.User_ID, val.Article_ID], val.Rating))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.07008214],\n",
       "       [ 1.28627229],\n",
       "       [-0.33980882],\n",
       "       ..., \n",
       "       [-0.1143198 ],\n",
       "       [ 1.66368413],\n",
       "       [ 0.91138458]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict([val.User_IDer_ID, val.Article_ID])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45939, 50)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_weights()[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Make the network deeper by adding dense layers and re-train the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of taking a dot product between user embeddings and article embeddings, we could use these embedding features, concatenate them and use them as a feature set for any downstream Machine Learning algorithm that is differentiable. A logistic regression / NN that have a differentiable loss function is a perfect fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 494544 samples, validate on 123893 samples\n",
      "Epoch 1/10\n",
      "494544/494544 [==============================] - 12s - loss: 4.3231 - rmse: 1.8573 - val_loss: 3.3557 - val_rmse: 1.8315\n",
      "Epoch 2/10\n",
      "494544/494544 [==============================] - 12s - loss: 4.1132 - rmse: 1.8063 - val_loss: 3.2994 - val_rmse: 1.8161\n",
      "Epoch 3/10\n",
      "494544/494544 [==============================] - 12s - loss: 4.0543 - rmse: 1.7820 - val_loss: 3.2691 - val_rmse: 1.8077\n",
      "Epoch 4/10\n",
      "494544/494544 [==============================] - 12s - loss: 4.0566 - rmse: 1.7646 - val_loss: 3.2550 - val_rmse: 1.8038\n",
      "Epoch 5/10\n",
      "494544/494544 [==============================] - 12s - loss: 4.0793 - rmse: 1.7464 - val_loss: 3.2675 - val_rmse: 1.8072\n",
      "Epoch 6/10\n",
      "494544/494544 [==============================] - 12s - loss: 4.1421 - rmse: 1.7197 - val_loss: 3.2682 - val_rmse: 1.8074\n",
      "Epoch 7/10\n",
      "494544/494544 [==============================] - 12s - loss: 4.1403 - rmse: 1.6769 - val_loss: 3.3129 - val_rmse: 1.8198\n",
      "Epoch 8/10\n",
      "494544/494544 [==============================] - 12s - loss: 4.2026 - rmse: 1.6422 - val_loss: 3.4295 - val_rmse: 1.8515\n",
      "Epoch 9/10\n",
      "494544/494544 [==============================] - 12s - loss: 4.2360 - rmse: 1.6265 - val_loss: 3.3886 - val_rmse: 1.8404\n",
      "Epoch 10/10\n",
      "494544/494544 [==============================] - 12s - loss: 4.2375 - rmse: 1.6110 - val_loss: 3.4590 - val_rmse: 1.8595\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3095bb08d0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_in, u = embedding_input('user_in', n_users, n_factors, 1e-3)\n",
    "article_in, a = embedding_input('article_in', n_articles, n_factors, 1e-3)\n",
    "\n",
    "x = merge([u, a], mode='concat')\n",
    "x = Flatten()(x)\n",
    "\n",
    "# Dense connections\n",
    "# x = Dropout(0.5)(x)\n",
    "x = Dense(500, activation='relu')(x)\n",
    "# x = Dropout(0.75)(x)\n",
    "x = Dense(1)(x)\n",
    "\n",
    "model = Model([user_in, article_in], x)\n",
    "model.compile(Adam(5e-3), loss='mse', metrics=[rmse])\n",
    "\n",
    "model.fit([trn.User_ID, trn.Article_ID], trn.Rating,\n",
    "          nb_epoch=10, batch_size=2048,\n",
    "          validation_data=([val.User_ID, val.Article_ID], val.Rating))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 494544 samples, validate on 123893 samples\n",
      "Epoch 1/10\n",
      "494544/494544 [==============================] - 13s - loss: 4.3133 - rmse: 1.8536 - val_loss: 3.3449 - val_rmse: 1.8285\n",
      "Epoch 2/10\n",
      "494544/494544 [==============================] - 13s - loss: 4.0634 - rmse: 1.8079 - val_loss: 3.2976 - val_rmse: 1.8156\n",
      "Epoch 3/10\n",
      "494544/494544 [==============================] - 13s - loss: 4.0283 - rmse: 1.7900 - val_loss: 3.2680 - val_rmse: 1.8074\n",
      "Epoch 4/10\n",
      "494544/494544 [==============================] - 13s - loss: 4.0492 - rmse: 1.7754 - val_loss: 3.2561 - val_rmse: 1.8041\n",
      "Epoch 5/10\n",
      "494544/494544 [==============================] - 13s - loss: 4.0971 - rmse: 1.7616 - val_loss: 3.2625 - val_rmse: 1.8059\n",
      "Epoch 6/10\n",
      "494544/494544 [==============================] - 13s - loss: 4.0867 - rmse: 1.7439 - val_loss: 3.2544 - val_rmse: 1.8036\n",
      "Epoch 7/10\n",
      "494544/494544 [==============================] - 13s - loss: 4.1635 - rmse: 1.7159 - val_loss: 3.2716 - val_rmse: 1.8084\n",
      "Epoch 8/10\n",
      "494544/494544 [==============================] - 13s - loss: 4.1819 - rmse: 1.6678 - val_loss: 3.3333 - val_rmse: 1.8254\n",
      "Epoch 9/10\n",
      "494544/494544 [==============================] - 13s - loss: 4.1902 - rmse: 1.6376 - val_loss: 3.3860 - val_rmse: 1.8398\n",
      "Epoch 10/10\n",
      "494544/494544 [==============================] - 13s - loss: 4.2529 - rmse: 1.6249 - val_loss: 3.4469 - val_rmse: 1.8562\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3073281350>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_in, u = embedding_input('user_in', n_users, n_factors, 1e-3)\n",
    "article_in, a = embedding_input('article_in', n_articles, n_factors, 1e-3)\n",
    "\n",
    "x = merge([u, a], mode='concat')\n",
    "x = Flatten()(x)\n",
    "\n",
    "# Dense connections\n",
    "# x = Dropout(0.5)(x)\n",
    "x = Dense(1000, activation='relu')(x)\n",
    "# x = Dropout(0.75)(x)\n",
    "x = Dense(1)(x)\n",
    "\n",
    "model = Model([user_in, article_in], x)\n",
    "model.compile(Adam(5e-3), loss='mse', metrics=[rmse])\n",
    "\n",
    "model.fit([trn.User_ID, trn.Article_ID], trn.Rating,\n",
    "          nb_epoch=10, batch_size=2048,\n",
    "          validation_data=([val.User_ID, val.Article_ID], val.Rating))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check summary and embeddings !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(45939, 50), (253933, 50), (45939, 1), (253933, 1)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x.shape for x in model.get_weights()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using side information\n",
    "\n",
    "Often, along with the user-interaction data, other information such as user metadata and item metadata is also given. With the above networks, it's trivial to add this metadata to our model. Let's see how."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(679051, 4)\n",
      "(291023, 3)\n",
      "(618437, 6)\n",
      "(618437, 9)\n",
      "(291023, 5)\n",
      "(291023, 8)\n",
      "(618437, 9)\n",
      "(291023, 9)\n"
     ]
    }
   ],
   "source": [
    "# 2. Read the necessary datasets\n",
    "user = pd.read_csv(\"../input/user.csv\")\n",
    "article = pd.read_csv(\"../input/article.csv\")\n",
    "train = pd.read_csv(\"../input/train.csv\"); print(train.shape)\n",
    "test = pd.read_csv(\"../input/test.csv\"); print(test.shape)\n",
    "\n",
    "diff = np.setdiff1d(train.User_ID.unique(), test.User_ID.unique())\n",
    "train = train[~train.User_ID.isin(diff)].reset_index(drop=True) # Drop train-only users.\n",
    "\n",
    "train = train.merge(user, how='left'); print(train.shape)\n",
    "train = train.merge(article, how='left'); print(train.shape)\n",
    "test = test.merge(user, how='left'); print(test.shape)\n",
    "test = test.merge(article, how='left'); print(test.shape)\n",
    "\n",
    "\n",
    "\n",
    "# For simplicity, impute with 0.\n",
    "# Ideally, you should either do mean / median imputation for numeric vars & mode imputation for cat vars.\n",
    "train = train.fillna(0)\n",
    "test = test.fillna(0)\n",
    "\n",
    "\n",
    "# 3. Create the interactions frame\n",
    "ratings = pd.concat([train, test])\n",
    "\n",
    "# Scaling numeric columns\n",
    "from sklearn.preprocessing import scale\n",
    "ratings.VintageMonths = scale(ratings.VintageMonths)\n",
    "\n",
    "users = ratings.User_ID.unique()\n",
    "articles = ratings.Article_ID.unique()\n",
    "age = ratings.Age.unique()\n",
    "var1 = ratings.Var1.unique()\n",
    "\n",
    "# Create userid & itemid to index mappings\n",
    "userid2idx = {o:i for i,o in enumerate(users)}\n",
    "articlesid2idx = {o:i for i,o in enumerate(articles)}\n",
    "age2idx = {o:i for i,o in enumerate(age)}\n",
    "var12idx = {o:i for i,o in enumerate(var1)}\n",
    "\n",
    "ratings.Article_ID = ratings.Article_ID.apply(lambda x: articlesid2idx[x])\n",
    "ratings.User_ID = ratings.User_ID.apply(lambda x: userid2idx[x])\n",
    "ratings.Age = ratings.Age.apply(lambda x: age2idx[x])\n",
    "ratings.Var1 = ratings.Var1.apply(lambda x: var12idx[x])\n",
    "\n",
    "n_users = ratings.User_ID.nunique()\n",
    "n_articles = ratings.Article_ID.nunique()\n",
    "n_age = ratings.Age.nunique()\n",
    "n_var1 = ratings.Var1.nunique()\n",
    "\n",
    "\n",
    "# 4. Split the frame into train and validation sets\n",
    "X_train = ratings[0:len(train)]; print(X_train.shape)\n",
    "X_test = ratings[len(train):len(ratings)]; print(X_test.shape)\n",
    "\n",
    "# Split the data into train and validation sets\n",
    "np.random.seed(2017)\n",
    "msk = np.random.rand(len(X_train)) < 0.8\n",
    "trn = X_train[msk]\n",
    "val = X_train[~msk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User_ID</th>\n",
       "      <th>Article_ID</th>\n",
       "      <th>Rating</th>\n",
       "      <th>ID</th>\n",
       "      <th>Var1</th>\n",
       "      <th>Age</th>\n",
       "      <th>VintageMonths</th>\n",
       "      <th>NumberOfArticlesBySameAuthor</th>\n",
       "      <th>NumberOfArticlesinSameCategory</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20080828074</td>\n",
       "      <td>1219102233</td>\n",
       "      <td>0</td>\n",
       "      <td>20080828074_1219102233</td>\n",
       "      <td>A</td>\n",
       "      <td>30-40</td>\n",
       "      <td>25.0</td>\n",
       "      <td>88</td>\n",
       "      <td>289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20080820760</td>\n",
       "      <td>1219151095</td>\n",
       "      <td>0</td>\n",
       "      <td>20080820760_1219151095</td>\n",
       "      <td>A</td>\n",
       "      <td>30-40</td>\n",
       "      <td>23.0</td>\n",
       "      <td>156</td>\n",
       "      <td>187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20080824760</td>\n",
       "      <td>1219295837</td>\n",
       "      <td>5</td>\n",
       "      <td>20080824760_1219295837</td>\n",
       "      <td>A</td>\n",
       "      <td>20-30</td>\n",
       "      <td>9.0</td>\n",
       "      <td>3</td>\n",
       "      <td>159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20080820470</td>\n",
       "      <td>1219098705</td>\n",
       "      <td>0</td>\n",
       "      <td>20080820470_1219098705</td>\n",
       "      <td>A</td>\n",
       "      <td>0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>43</td>\n",
       "      <td>503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20080821438</td>\n",
       "      <td>1219144384</td>\n",
       "      <td>0</td>\n",
       "      <td>20080821438_1219144384</td>\n",
       "      <td>A</td>\n",
       "      <td>30-40</td>\n",
       "      <td>17.0</td>\n",
       "      <td>39</td>\n",
       "      <td>264</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       User_ID  Article_ID  Rating                      ID Var1    Age  \\\n",
       "0  20080828074  1219102233       0  20080828074_1219102233    A  30-40   \n",
       "1  20080820760  1219151095       0  20080820760_1219151095    A  30-40   \n",
       "2  20080824760  1219295837       5  20080824760_1219295837    A  20-30   \n",
       "3  20080820470  1219098705       0  20080820470_1219098705    A      0   \n",
       "4  20080821438  1219144384       0  20080821438_1219144384    A  30-40   \n",
       "\n",
       "   VintageMonths  NumberOfArticlesBySameAuthor\\r  \\\n",
       "0           25.0                              88   \n",
       "1           23.0                             156   \n",
       "2            9.0                               3   \n",
       "3           19.0                              43   \n",
       "4           17.0                              39   \n",
       "\n",
       "   NumberOfArticlesinSameCategory\\r  \n",
       "0                               289  \n",
       "1                               187  \n",
       "2                               159  \n",
       "3                               503  \n",
       "4                               264  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add age and other numeric variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 494544 samples, validate on 123893 samples\n",
      "Epoch 1/10\n",
      "494544/494544 [==============================] - 14s - loss: 5.4007 - rmse: 1.9394 - val_loss: 3.2666 - val_rmse: 1.8070\n",
      "Epoch 2/10\n",
      "494544/494544 [==============================] - 14s - loss: 3.3861 - rmse: 1.7655 - val_loss: 3.2317 - val_rmse: 1.7973\n",
      "Epoch 3/10\n",
      "494544/494544 [==============================] - 14s - loss: 3.2556 - rmse: 1.6934 - val_loss: 3.2974 - val_rmse: 1.8155\n",
      "Epoch 4/10\n",
      "494544/494544 [==============================] - 14s - loss: 3.1317 - rmse: 1.6294 - val_loss: 3.3972 - val_rmse: 1.8428\n",
      "Epoch 5/10\n",
      "494544/494544 [==============================] - 14s - loss: 3.0245 - rmse: 1.5802 - val_loss: 3.4557 - val_rmse: 1.8585\n",
      "Epoch 6/10\n",
      "494544/494544 [==============================] - 14s - loss: 2.9564 - rmse: 1.5485 - val_loss: 3.5138 - val_rmse: 1.8741\n",
      "Epoch 7/10\n",
      "494544/494544 [==============================] - 14s - loss: 2.9017 - rmse: 1.5239 - val_loss: 3.5517 - val_rmse: 1.8842\n",
      "Epoch 8/10\n",
      "494544/494544 [==============================] - 14s - loss: 2.8516 - rmse: 1.5024 - val_loss: 3.5840 - val_rmse: 1.8927\n",
      "Epoch 9/10\n",
      "494544/494544 [==============================] - 14s - loss: 2.8066 - rmse: 1.4847 - val_loss: 3.6209 - val_rmse: 1.9024\n",
      "Epoch 10/10\n",
      "494544/494544 [==============================] - 14s - loss: 2.7734 - rmse: 1.4714 - val_loss: 3.6492 - val_rmse: 1.9098\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f308b2a63d0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO\n",
    "user_in, u = embedding_input('user_in', n_users, n_factors, 1e-3)\n",
    "article_in, a = embedding_input('article_in', n_articles, n_factors, 1e-3)\n",
    "\n",
    "meta_input_f0 = Input(shape=[1], name='meta_input_f0') # Age\n",
    "meta_input_f1 = Input(shape=[1], name='meta_input_f1') # NumberOfArticlesBySameAuthor\n",
    "meta_input_f2 = Input(shape=[1], name='meta_input_f2') # NumberOfArticlesinSameCategory\n",
    "meta_input_f3 = Input(shape=[1], name='meta_input_f3') # VintageMonths\n",
    "meta_input_f4 = Input(shape=[1], name='meta_input_f4') # Var1\n",
    "\n",
    "x = merge([u, a], mode='concat')\n",
    "x = Flatten()(x)\n",
    "\n",
    "# Dense connections\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(1000, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(1)(x)\n",
    "\n",
    "model = Model([user_in, article_in, meta_input_f0, meta_input_f1, meta_input_f2, meta_input_f3, meta_input_f4], x)\n",
    "model.compile(Adam(5e-4), loss='mse', metrics=[rmse])\n",
    "\n",
    "model.fit([trn.User_ID, trn.Article_ID, trn.Age, trn['NumberOfArticlesBySameAuthor\\r'], trn['NumberOfArticlesinSameCategory\\r'], trn['VintageMonths'], trn['Var1']], trn.Rating,\n",
    "          nb_epoch=10, batch_size=2048,\n",
    "          validation_data=([val.User_ID, val.Article_ID, val.Age, val['NumberOfArticlesBySameAuthor\\r'], val['NumberOfArticlesinSameCategory\\r'], val['VintageMonths'], val['Var1']], val.Rating))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# But, what if you don't have ratings!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do you need ratings? I have a lot of logs, API hits, app clickstream data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credit where it's due\n",
    "- A brilliant [fast.ai](course.fast.ai) course by Jeremy and Rachel. Refer to Lesson 4 for Collaborative Filtering lecture.\n",
    "- Scikit-Learn core member [class](https://m2dsupsdlclass.github.io/lectures-labs/) on deep learning. \n",
    "- Reference: Keras [Merge Layer](https://faroit.github.io/keras-docs/1.0.4/getting-started/sequential-model-guide/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For those who'd like to get *deeper*\n",
    "- Deep Recommender models using PyTorch - [Spotlight](https://github.com/maciejkula/spotlight). The [Keras](https://github.com/maciejkula/triplet_recommendations_keras) implementation.\n",
    "- [YouTube Recommendation Engine](http://static.googleusercontent.com/media/research.google.com/en//pubs/archive/45530.pdf)  (Combination of techniques)\n",
    "- [Google Play Apps Recommendations Engine](https://arxiv.org/pdf/1606.07792.pdf)\n",
    "- RecSys conference 2017 had a lot of [talks](https://towardsdatascience.com/recsys-2017-2d0879351097) where deep learning was the primary theme. Official reviews [here](https://medium.com/@ACMRecSys/recsys2017-summaries-and-reviews-f2bea3f0e519)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
